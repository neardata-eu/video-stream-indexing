{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/agabriel/venvs/lithops/lib/python3.9/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.dirname(\"/home/agabriel/repos/video-stream-indexing/scripts/query/\"), '../..')))\n",
    "\n",
    "from pymilvus import (\n",
    "    connections,\n",
    "    MilvusClient\n",
    ")\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import argparse\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from functools import partial\n",
    "\n",
    "from policies.constants import (MILVUS_HOST, MILVUS_PORT, MILVUS_NAMESPACE,\n",
    "                                LOG_PATH, RESULT_PATH)\n",
    "from policies.components import get_model, inference\n",
    "\n",
    "from index_utils import search, search_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorDataset(Dataset):\n",
    "    def __init__(self, milvus, embedding, global_k, global_accuracy, global_f, local_k, fragment_offset, accuracy, result_path, frame_path, parallelism_candidates, parallelism_exports):\n",
    "        # Search Global Index for candidates\n",
    "        candidates, _ = search_global(\"global\", embedding.detach().numpy(), [\"collection\"], int(global_k), float(global_accuracy), global_f)\n",
    "        \n",
    "        # Generate video fragments\n",
    "        fragments = []\n",
    "        with ThreadPoolExecutor(max_workers=int(parallelism_candidates)) as executor:\n",
    "            search_partial = partial(\n",
    "                search, \n",
    "                milvus_client=milvus, \n",
    "                embedding=embedding.detach().numpy(), \n",
    "                fields=[\"offset\", \"pk\"], \n",
    "                local_k=int(local_k),\n",
    "                fragment_offset=int(fragment_offset), \n",
    "                accuracy=float(accuracy), \n",
    "                result_path=result_path,\n",
    "                parallelism=int(parallelism_exports)\n",
    "            )\n",
    "            futures = {executor.submit(search_partial, collection_name=collection_name): collection_name for collection_name in candidates}\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                collection_name = futures[future]\n",
    "                try:\n",
    "                    fragment, _, _ = future.result()\n",
    "                    fragments.extend(fragment)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing collection {collection_name}: {e}\")\n",
    "\n",
    "        # Save individual images on disk\n",
    "        self.frame_path = frame_path # Path to store the frame .jpgs\n",
    "        frame_count = 0\n",
    "        print(\"Generating dataset...\")\n",
    "        for file in fragments:\n",
    "            cap = cv2.VideoCapture(f\"{result_path}/{file}\")\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                cv2.imwrite(f\"{frame_path}/frame_{frame_count}.jpg\", frame)\n",
    "                frame_count += 1\n",
    "            cap.release()\n",
    "        self.frame_count = frame_count # Dataset length\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.frame_count\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(f\"{self.frame_path}/frame_{idx}.jpg\")\n",
    "        image_array = np.array(image)\n",
    "        return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to Milvus\n",
      "Total hits: 60\n",
      "Collection: 0.00458836555480957\n",
      "Load: 0.0035758018493652344\n",
      "Search: 0.002328157424926758\n",
      "Filter1: 9.131431579589844e-05\n",
      "Filter2: 1.7881393432617188e-05\n",
      "Searching in col02\n",
      "Exporting fragments from col02\n",
      "An error occurred: Command '['bash', '/project/scripts/query/export.sh', 'col02', '/project/video_fragments/col02_0_173647945_179926018.h264', '173647945', '179926018']' returned non-zero exit status 127.\n",
      "An error occurred: Command '['bash', '/project/scripts/query/export.sh', 'col02', '/project/video_fragments/col02_1_197199240_202011079.h264', '197199240', '202011079']' returned non-zero exit status 127.\n",
      "An error occurred: Command '['bash', '/project/scripts/query/export.sh', 'col02', '/project/video_fragments/col02_2_204066934_206771498.h264', '204066934', '206771498']' returned non-zero exit status 127.\n",
      "An error occurred: Command '['bash', '/project/scripts/query/export.sh', 'col02', '/project/video_fragments/col02_3_210965401_213760072.h264', '210965401', '213760072']' returned non-zero exit status 127.\n",
      "Generating dataset...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(np\u001b[38;5;241m.\u001b[39marray(img, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8))\n\u001b[1;32m     47\u001b[0m         img\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/last_frame_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m## Create the dataset and dataloader\u001b[39;00m\n\u001b[1;32m     21\u001b[0m dataset \u001b[38;5;241m=\u001b[39m VectorDataset(\n\u001b[1;32m     22\u001b[0m     milvus\u001b[38;5;241m=\u001b[39mclient,\n\u001b[1;32m     23\u001b[0m     embedding\u001b[38;5;241m=\u001b[39membedding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m     frame_path\u001b[38;5;241m=\u001b[39mframe_path\n\u001b[1;32m     34\u001b[0m )\n\u001b[0;32m---> 35\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m## Example use\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n",
      "File \u001b[0;32m~/venvs/lithops/lib/python3.9/site-packages/torch/utils/data/dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/lithops/lib/python3.9/site-packages/torch/utils/data/sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    result_path = \"/home/agabriel/repos/video-stream-indexing/results\"\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    frame_path = f\"{result_path}/frames\"\n",
    "    os.makedirs(frame_path, exist_ok=True)\n",
    "    \n",
    "    ## Connect to Milvus\n",
    "    print(\"Connecting to Milvus\")\n",
    "    connections.connect(\"default\", host=\"localhost\", port=19530)\n",
    "    client = MilvusClient(uri=f\"http://localhost:19530\")\n",
    "    \n",
    "    ## Read image and perform inference\n",
    "    model, device = get_model()\n",
    "    img = Image.open(\"/home/agabriel/repos/video-stream-indexing/benchmarks/experiment3/cholec_frame_ref.png\")\n",
    "    if img.mode == 'RGBA':\n",
    "        img = img.convert('RGB')\n",
    "    img.resize((940, 560))\n",
    "    embedding = inference(model, img, device)\n",
    "\n",
    "    ## Create the dataset and dataloader\n",
    "    dataset = VectorDataset(\n",
    "        milvus=client,\n",
    "        embedding=embedding,\n",
    "        global_k=3,\n",
    "        global_accuracy=0.0,\n",
    "        global_f=20,\n",
    "        local_k=20,\n",
    "        fragment_offset=5,\n",
    "        accuracy=0.8,\n",
    "        result_path=RESULT_PATH,\n",
    "        parallelism_candidates=3,\n",
    "        parallelism_exports=3,\n",
    "        frame_path=frame_path\n",
    "    )\n",
    "    dataloader = DataLoader(dataset, batch_size=5, shuffle=True)\n",
    "\n",
    "    ## Example use\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        print(f\"Batch {idx} with shape: {batch.shape}\")\n",
    "        last = batch\n",
    "        break\n",
    "        \n",
    "    # Display last batch in separate window\n",
    "    for idx, img in enumerate(last):\n",
    "        # Save to disk\n",
    "        img = Image.fromarray(np.array(img, dtype=np.uint8))\n",
    "        img.save(f\"{result_path}/last_frame_{idx}.jpg\")\n",
    "    \n",
    "        \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lithops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
